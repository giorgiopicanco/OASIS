#!/usr/bin/env python3
import datetime as dt
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from astropy.time import Time
from scipy.constants import speed_of_light
from scipy.ndimage import uniform_filter1d
from numpy.polynomial import Polynomial
from sklearn.preprocessing import StandardScaler
from matplotlib import font_manager
import georinex as gr
import sys
import time
from settings import *
from linear_combinations import *
from pyproj import Transformer, Proj, transform
import gnss_freqs
import warnings

# Accessing the frequencies of the GPS system
gps_freqs = gnss_freqs.FREQUENCY[gnss_freqs.GPS]
f1 = gps_freqs[1]
f2 = gps_freqs[2]
f5 = gps_freqs[5]

# Calculating the frequencies of the GLONASS system (glonass_channels.dat)
# To obtain the GLONASS frequencies (code 'R'):
file_name = 'glonass_channels.dat'

# Read the file into a DataFrame with column names defined
df_slots = pd.read_csv(file_name, sep=' ', header=None, names=['Slot', 'Channel'])
glonass_frequencies = gnss_freqs.FREQUENCY[gnss_freqs.GLO]

# List to store the data
data = []

# Iterate over each row of the DataFrame
for index, row in df_slots.iterrows():
    satellite = row['Slot']
    k = row['Channel']
    row_data = [satellite]

    for channel, frequency in glonass_frequencies.items():
        if callable(frequency):  # Check if it is a lambda function
            freq_value = frequency(k)
        else:
            freq_value = frequency
        formatted_freq = f"{freq_value:.1f}"
        row_data.append(formatted_freq)
    data.append(row_data)

# Convert the list of lists into a pandas DataFrame
glo_freqs = pd.DataFrame(data, columns=['Satellite', 'fr1', 'fr2', 'fr3'])

# Initial time, number of hours, and xticks interval on the graph.
h1 = 0
n_horas = 24
int1 = 120

# Minimum number of observations per arc
ARCL = 15

# Arguments from the management program
station_name = sys.argv[1]
day_of_year = sys.argv[2]
year = sys.argv[3]
output_folder = sys.argv[4]
input_folder = sys.argv[5]
orbit_folder = sys.argv[6]

constellations = ['G', 'R']  # G: GPS, R: GLONASS, E: Galileo, C: BeiDou

for c in constellations:
    sat_class = c

    # RINEX OPENING:
    # ------------------------------------
    # Selection of version for RINEX files
    version_number = '1'
    print()
    year_format = f"{year[-2:]}o"
    rinex_file_path = f"{input_folder}/{station_name}{day_of_year}{version_number}.{year_format}"
    if not os.path.exists(rinex_file_path):
        version_number = '0' if version_number == '1' else '1'
        rinex_file_path = f"{input_folder}/{station_name}{day_of_year}{version_number}.{year_format}"
        if not os.path.exists(rinex_file_path):
            print(f"File {rinex_file_path} not found either. Please check the path or other issues.")
        else:
            print(f"File {rinex_file_path} found with opposite version of {version_number}.")
    else:
        print(f"File {rinex_file_path} found with version_number = {version_number}")

    # Supressing warning about timedelta
    warnings.filterwarnings("ignore", message="Converting non-nanosecond precision datetime values to nanosecond precision.", category=UserWarning)
    obs_data = gr.load(rinex_file_path)
    print(obs_data)
    print()

    # ORBIT FILE OPENING:
    # -----------------------------
    # Path and time definitions

    freq = int(obs_data.interval)
    file_path = os.path.join(orbit_folder, f'ORBITS_{year}_{day_of_year}_{freq}S.SP3')
    column_names = ["Date", "Time", "Satélite", "X", "Y", "Z"]
    df2 = pd.read_csv(file_path, sep="\t", header=0, names=column_names)

    # List to store IPP coordinates
    result = {"Date": [], "Time": [], "SAT": [], "lon": [], "lat": [], "el": []}
    index = []




    print(f"Constellation: {sat_class}")

    # Filtering satellites by class
    satellites_to_plot = [sv for sv in np.unique(obs_data.sv.values) if sv.startswith(sat_class)]
    print(f"Satellites to plot for {sat_class}: {satellites_to_plot}")

    # List to store DataFrames for each satellite
    dfs = []

    # Remove the first character of each element in the 'Satellite' column
    df2['Satélite'] = df2['Satélite'].str[1:]

    IPP_UNIQ = np.unique(df2['Satélite'])
    print(IPP_UNIQ)

    # Convertendo listas para conjuntos
    set_IPP_UNIQ = set(IPP_UNIQ)
    set_satellites_to_plot = set(satellites_to_plot)

    # Encontrando a interseção entre os dois conjuntos
    common_elements = set_IPP_UNIQ.intersection(set_satellites_to_plot)

    # Convertendo o conjunto de elementos comuns de volta para uma lista, se necessário
    common_elements_list = list(common_elements)

    # Definindo a ordem desejada para as letras iniciais
    order = {'G': 1, 'R': 2, 'E': 3, 'C': 4}

    # Ordenando a lista com base na letra inicial e nos três últimos números
    sorted_common_elements_list = sorted(common_elements_list, key=lambda x: (order[x[0]], int(x[-2:])))

    # Imprimindo a lista ordenada
    print('Common sattelites:', sorted_common_elements_list)

    for sat in sorted_common_elements_list:
        print()
        print("Processando satélite:", sat)
        print()

        # Verificando a classe do satélite e ajustando os valores de f1, f2, f5
        if sat_class == 'G':
            f1 = f1
            f2 = f2
            f5 = f5
        elif sat_class == 'R':
            # Localizar a linha onde 'Satellite' é igual a 'sat'
            sat_row = glo_freqs.loc[glo_freqs['Satellite'] == sat]

            if not sat_row.empty:
                f1 = float(sat_row['fr1'].values[0])
                f2 = float(sat_row['fr2'].values[0])
                f5 = float(sat_row['fr3'].values[0])
            else:
                f1 = f2 = f5 = None  # Ou valores padrão

        # Exibindo os valores
        print(f"f1: {f1}")
        print(f"f2: {f2}")
        print(f"f5: {f5}")

        L1 = np.array(obs_data['L1'].sel(sv=sat))
        L2 = np.array(obs_data['L2'].sel(sv=sat))

        # Adicionando a frequência 5 para L1 e L2
        if 'L5' in obs_data:
            L5 = np.array(obs_data['L5'].sel(sv=sat))
        else:
            L5 = np.full_like(L1, np.nan)  # Preenche com NaN se não houver dados para L5

        if 'P1' in obs_data:
            P1 = np.array(obs_data['P1'].sel(sv=sat))
            code_obs1 = 'P1'
        else:
            P1 = np.array(obs_data['C1'].sel(sv=sat))
            code_obs1 = 'C1'
        if np.all(np.isnan(P1)):
            P1 = np.array(obs_data['C1'].sel(sv=sat))
            code_obs1 = 'C1'
        else:
            code_obs1 = 'P1'
        if 'P2' in obs_data:
            P2 = np.array(obs_data['P2'].sel(sv=sat))
            code_obs2 = 'P2'
        else:
            P2 = np.array(obs_data['C2'].sel(sv=sat))
            code_obs2 = 'C2'
        if np.all(np.isnan(P2)):
            P2 = np.array(obs_data['C2'].sel(sv=sat))
            code_obs2 = 'C2'
        else:
            code_obs2 = 'P2'

        # Verificando se há observações de código de portadora (P5) ou pseudorrange (C5)
        if 'P5' in obs_data:
            P5 = np.array(obs_data['P5'].sel(sv=sat))
            code_obs5 = 'P5'
            if np.all(np.isnan(P5)):
                P5 = np.array(obs_data['C5'].sel(sv=sat))
                code_obs5 = 'C5'
                if np.all(np.isnan(P5)):
                    # print(f"WARNING: All values in {code_obs5} are NaNs for satellite {sat}")
                    print()
        elif 'C5' in obs_data:
            P5 = np.array(obs_data['C5'].sel(sv=sat))
            code_obs5 = 'C5'
            if np.all(np.isnan(P5)):
                # print(f"WARNING: All values in {code_obs5} are NaNs for satellite {sat}")
                print()
        else:
            # Se nem P5 nem C5 estiverem presentes, definir P5 como uma coluna de NaNs
            P5 = np.full_like(L5, np.nan)
            code_obs5 = "None"  # Ou qualquer valor de indicação de ausência de dados que você preferir

        if np.all(np.isnan(P5)):
            print(f"WARNING: All values in L5 data", f"{code_obs5} are NaNs for satellite {sat}")

        df = pd.DataFrame({'time': obs_data.time})
        df.set_index('time', inplace=True)
        df['index'] = df.index
        df['mjd'] = Time(df.index).mjd
        mjd = Time(df.index).mjd

        df['timestamp'] = pd.to_datetime(df.index)

        # Extraindo apenas o tempo de 'timestamp' e armazenando em uma nova coluna 'time'
        df['time'] = df['timestamp'].dt.time
        df['date'] = df['timestamp'].dt.date

        df['L1'] = L1
        df['L2'] = L2
        df['L5'] = L5
        df['P1'] = P1
        df['P2'] = P2
        df['P5'] = P5

        df['satellite'] = [sat] * len(L1)
        df['station'] = [station_name.upper()] * len(L1)
        df['position'] = [obs_data.position] * len(L1)
        df['mjd'] = ["{:.12f}".format(valor) for valor in mjd]

        timep = df['time']

        L1, L2, L5, P1, P2, P5 = df['L1'] , df['L2'] , df['L5'] , df['P1'] , df['P2'], df['P5']

        mjd, date, time = df['mjd'] , df['date'] , df['time']

        df['LMW12'] = melbourne_wubbena(f1, f2, L1, L2, P1, P2)

        df['LMW15'] = melbourne_wubbena(f1, f5, L1, L5, P1, P5)

        # station_coords = df['position'][0]
        station_coords = df['position'].iloc[0]

        # Converter cada string da lista em um float (ou int, se adequado)
        coords_list = [float(coord) for coord in station_coords]

        # Convertendo a lista de coordenadas em uma lista contendo uma única tupla
        obs_x, obs_y, obs_z = coords_list[0], coords_list[1], coords_list[2]

        print( obs_x, obs_y, obs_z )

        # Inicialize listas vazias para coletar dados
        all_data = []
        all_time = []
        all_sat = []
        all_longitude = []
        all_latitude = []
        all_elevation = []

        # for ipp_sat in sorted_common_elements_list:
        indices = np.where(df2['Satélite'] == sat)[0]
        df_filtrado = df2.iloc[indices]
        print(sat)

        for _, row in df_filtrado.iterrows():
            date = row["Date"]
            sat = row["Satélite"]
            sx = row["X"]
            sy = row["Y"]
            sz = row["Z"]
            time = row["Time"]

            # print("Processando PRN:", sat)

            # Supondo que convert_coords e IonosphericPiercingPoint são funções definidas anteriormente
            lon, lat, alt = convert_coords(obs_x, obs_y, obs_z, to_radians=True)
            ip = IonosphericPiercingPoint(sx, sy, sz, obs_x, obs_y, obs_z)
            elevation = ip.elevation(lat, lon)
            lat_ip, lon_ip = ip.coordinates(lat, lon)

            # Adicione os dados às listas
            all_data.append(date)
            all_time.append(time)
            all_sat.append(sat)
            all_longitude.append(lon_ip)
            all_latitude.append(lat_ip)
            all_elevation.append(elevation)

        # Após sair do loop, converta as listas para NumPy arrays para facilitar a manipulação (se necessário)
        all_date = np.array(all_data)
        all_time = np.array(all_time)
        all_sat = np.array(all_sat)
        all_longitude = np.array(all_longitude)
        all_latitude = np.array(all_latitude)
        all_elevation = np.array(all_elevation)

        # Criar um novo DataFrame combinado
        combined_df = pd.DataFrame({
            "Date": all_date,
            "Time": all_time,
            "SAT": all_sat,
            "Longitude": all_longitude,
            "Latitude": all_latitude,
            "Elevation": all_elevation
        })


        # Convertendo os dados de hora para o tipo datetime
        combined_df['Time'] = pd.to_datetime(combined_df['Time'], format='%H:%M:%S')

        df['time'] = df['time'].astype(str)

        # Convertendo as horas para objetos datetime
        combined_df['Time'] = pd.to_datetime(combined_df['Time']).dt.time
        #df['time'] = pd.to_datetime(df['time']).dt.time

        # Especificar o formato ao converter para datetime
        df['time'] = pd.to_datetime(df['time'], format='%H:%M:%S').dt.time

        # Encontrando as horas que estão presentes em ambos os dataframes
        horas_em_common = set(combined_df['Time']).intersection(set(df['time']))

        # Definindo o comprimento de referência como o menor comprimento entre os dois dataframes
        comprimento_referencia = min(len(combined_df), len(df))

        # Filtrando os dataframes para manter apenas as linhas comuns e com base no comprimento de referência
        combined_df = combined_df[combined_df['Time'].isin(horas_em_common)].iloc[:comprimento_referencia]
        df = df[df['time'].isin(horas_em_common)].iloc[:comprimento_referencia]

        colunas_desejadas_df1 = ['Elevation','Longitude','Latitude']

        combined_df_selecionado = combined_df[colunas_desejadas_df1]

        L1, L2, L5, P1, P2, P5 = df['L1'] , df['L2'] , df['L5'], df['P1'] , df['P2'] , df['P5']

        mjd, date, time = df['mjd'] , df['date'] , df['time']

        # Supondo que df_filtered seja o seu DataFrame
        # Divide a coluna 'position' em três novas colunas e expande para o resultado ser colunas separadas
        df['pos_x'], df['pos_y'], df['pos_z'] = obs_x, obs_y, obs_z

        df['height'] = np.full(len(L1), 450.0)

        # Agora df_filtered tem as colunas 'pos_x', 'pos_y', e 'pos_z'
        # Você pode então selecionar estas novas colunas para inclusão no DataFrame que será salvo
        colunas_desejadas_df2 = ['date','time', 'mjd', 'pos_x', 'pos_y', 'pos_z', 'L1', 'L2', 'L5', 'P1', 'P2', 'P5', 'satellite','station','height']
        df_filtered_selecionado = df[colunas_desejadas_df2]

        # Certifique-se de que df_filtered e combined_df tenham o mesmo comprimento
        if len(df_filtered_selecionado) == len(combined_df_selecionado):
            # Juntando os DataFrames lado a lado
            df_final = pd.concat([df_filtered_selecionado, combined_df_selecionado], axis=1)
        else:
            print("Erro: df_filtered e combined_df não têm o mesmo comprimento!")
            # Sair da execução se os DataFrames não tiverem o mesmo tamanho
            sys.exit()

        LMW2 = df['LMW12']
        LMW3 = df['LMW15']

        abs_elevation = abs(combined_df_selecionado['Elevation'])

        df['LMW2'] = LMW2
        df['LMW3'] = LMW3

        combined_df_selecionado.index = df.index

        indices_low_elevation = combined_df_selecionado.index[abs_elevation < 10]

        # Lista de colunas que devem ser definidas como NaN
        cols_nan = ['LMW2', 'LMW3']

        # Atribuir NaN às colunas especificadas apenas para as linhas em indices_low_elevation
        df.loc[indices_low_elevation, cols_nan] = np.nan

        # Suponho que LMW seja um numpy array e time seja um vetor de tempos correspondente
        LMW = np.array(df['LMW2'])  # Asegurar que LMW seja um numpy array para facilitar a manipulação
        LMW15 = np.array(df['LMW3'])  # Asegurar que LMW seja um numpy array para facilitar a manipulação

        arcs = []  # Lista para armazenar os arcos de observação
        current_arc = []  # Lista temporária para armazenar o arco de observação atual

        # Iterar sobre todos os elementos de LMW
        for idx, value in enumerate(LMW):
            if np.isnan(value):
                # Se o valor atual for NaN, verificar se o arco atual está vazio
                # Isso evita adicionar arcos vazios caso haja NaNs consecutivos
                if current_arc:
                    arcs.append(current_arc)
                    current_arc = []
            else:
                # Se o valor não for NaN, adicionar o índice ao arco atual
                current_arc.append(idx)

        # Adicionar o último arco se ele não estiver vazio
        if current_arc:
            arcs.append(current_arc)

        print()
        print('Melbourne-Wubbena combination for L1-L2')
        print()

        # Imprimir informações de cada arco e classificá-los
        for i, arc in enumerate(arcs):
            start_index = arc[0]
            end_index = arc[-1]
            num_observations = len(arc)
            status = "Mantido" if num_observations >= 15 else "Descartado"

            print(f"Arco {i + 1}: Índice inicial = {start_index}, Índice final = {end_index}, "
                f"Número de observações = {num_observations}, Status = {status}")

        arcs15 = []  # Lista para armazenar os arcos de observação
        current_arc15 = []  # Lista temporária para armazenar o arco de observação atual

        # Iterar sobre todos os elementos de LMW
        for idx, value in enumerate(LMW15):
            if np.isnan(value):
                # Se o valor atual for NaN, verificar se o arco atual está vazio
                # Isso evita adicionar arcos vazios caso haja NaNs consecutivos
                if current_arc15:
                    arcs15.append(current_arc15)
                    current_arc15 = []
            else:
                # Se o valor não for NaN, adicionar o índice ao arco atual
                current_arc15.append(idx)

        # Adicionar o último arco se ele não estiver vazio
        if current_arc15:
            arcs15.append(current_arc15)

        print()
        print()

        print('Melbourne-Wubbena combination for L1-L5')

        print()

        # Imprimir informações de cada arco e classificá-los
        for i, arc in enumerate(arcs15):
            start_index = arc[0]
            end_index = arc[-1]
            num_observations = len(arc)
            status = "Mantido" if num_observations >= 15 else "Descartado"
            print(f"Arco {i + 1}: Índice inicial = {start_index}, Índice final = {end_index}, "
                f"Número de observações = {num_observations}, Status = {status}")

        LMW = np.array(LMW)  # Continuamos assumindo que LMW é seu array de dados
        LMW15 = np.array(LMW15)  # Continuamos assumindo que LMW é seu array de dados

        print()
        print()

        # Função para reescalar os dados para o intervalo [-10, 10]
        def rescale_data(data):
            min_val = np.min(data)
            max_val = np.max(data)
            # Reescala os dados para o intervalo [0, 1]
            scaled_data = (data - min_val) / (max_val - min_val)
            # Ajusta para o intervalo [-10, 10]
            final_data = scaled_data * 20 - 10
            return final_data


        # ----------------- [L1 - L2]

        idx_total = []

        for arc in arcs:
            arc_data = df.iloc[arc]
            time = df.index[arc]

            if len(arc_data) < ARCL:  # Verifica se há pontos suficientes para o ajuste
                continue

            x = arc_data.index.astype(np.int64) // 10**9  # Convertendo para segundos
            xx = arc_data['time']
            y = arc_data['LMW2'].values

            y_rescaled = rescale_data(y)
            delta_y = np.diff(y_rescaled, prepend=np.nan)

            # # Ajustar um polinômio apenas nos valores válidos (excluindo np.nan)
            p = Polynomial.fit(x[1:], delta_y[1:], 3)

            delta_y_fit = p(x)  # Valores ajustados pelo polinômio
            residuals = abs(delta_y - delta_y_fit)  # Calcular resíduos

            # Cálculo dos quartis e IQR para identificação de outliers
            Q1 = np.nanpercentile(residuals, 15)
            Q3 = np.nanpercentile(residuals, 85)
            IQR = Q3 - Q1
            outlier_mask = (residuals < Q1 - 4 * IQR) | (residuals > Q3 + 4 * IQR)
            high_residuals_mask = residuals > 1  # Máscara para resíduos altos
            other_residuals_mask = ~(outlier_mask | high_residuals_mask)  # Máscara para os demais resíduos

            # Configurar subplots
            fig, axs = plt.subplots(3, 1, figsize=(10, 15), sharex=True)

            # Primeiro subplot: dados originais reescalados
            axs[0].scatter(time, y_rescaled, label='Dados Reescalados')
            axs[0].set_title(f'Análise do Arco {arcs.index(arc) + 1} - Satélite: {sat} - L1-L2' )
            axs[0].set_ylabel('LMW 1-2 (Reescalado)')
            axs[0].legend()

            # Segundo subplot: diferenças ponto a ponto e polinômio ajustado
            axs[1].scatter(time, delta_y, color='green', label='Diferença Ponto a Ponto', marker='o')
            axs[1].plot(time, delta_y_fit, color='red', label='Polinômio de 3º Grau Ajustado')
            axs[1].axhline(0, color='gray', linestyle='--')
            axs[1].set_ylabel('Delta Y (Reescalado)')
            axs[1].legend()
            # plt.grid(axis='both', linestyle='--', color='black', linewidth=0.5)

            # Terceiro subplot: resíduos com outliers e resíduos altos
            axs[2].scatter(time[other_residuals_mask], residuals[other_residuals_mask], color='purple', label='Resíduos Normais', marker='o')
            axs[2].scatter(time[outlier_mask], residuals[outlier_mask], color='green', label='Outliers', marker='o')
            axs[2].scatter(time[high_residuals_mask], residuals[high_residuals_mask], color='red', label='Resíduos Altos (> 1)', marker='o')

            axs[0].scatter(time[outlier_mask], y_rescaled[outlier_mask], color='green', label='Outliers', marker='o')
            axs[0].scatter(time[high_residuals_mask], y_rescaled[high_residuals_mask], color='red', label='Resíduos Altos (> 1)', marker='o')
            # plt.grid(axis='both', linestyle='--', color='black', linewidth=0.5)

            axs[2].axhline(0, color='gray', linestyle='--')
            # axs[2].set_ylim(0, 10)  # Definir os limites do eixo y para os resíduos
            axs[2].set_xlabel('Tempo (s)')
            axs[2].set_ylabel('Resíduos')
            axs[2].legend()

            plt.grid(axis='both', linestyle='--', color='black', linewidth=0.5)
            hours_fmt = mdates.DateFormatter('%H:%M')
            minute_locator = mdates.MinuteLocator(interval=int1)
            plt.gca().xaxis.set_major_formatter(hours_fmt)
            plt.gca().xaxis.set_major_locator(minute_locator)
            hora_inicio = h1 * 240
            start_time = np.datetime64(obs_data.time[hora_inicio].to_numpy())
            # # plt.xlim([start_time, start_time + np.timedelta64(n_horas, 'h')])
            plt.tick_params(axis='both', which='major', labelsize=12)
            # plt.show()

            # Suponha que 'residuos' seja a variável que contém os resíduos calculados
            limiar_outlier = 4  # Limiar para identificar outliers (por exemplo, 4 vezes o IQR)
            limiar_residuo_alto = 1  # Limiar para identificar resíduos altos

            # Identificar índices de outliers e resíduos altos
            indices_outliers =  arc[0] + np.where(np.abs(residuals) > limiar_outlier * IQR)[0]
            indices_residuos_altos = arc[0] + np.where(np.abs(residuals) > limiar_residuo_alto)[0]

            # Juntar os índices sem repeti-los
            indices_combinados = np.union1d(indices_outliers, indices_residuos_altos)

            # print("Índices combinados:", indices_combinados)

            # Adicionar os valores de indices_combinados à lista
            idx_total.append(indices_combinados)

            # print("Índices combinados (L1 - L2):", indices_combinados)#, timep.iloc[indices_combinados])

        # Concatenar todos os valores da lista em uma única array numpy
        if idx_total:
            idx_total = np.concatenate(idx_total)

        # Inicializar 'flags' com 'S' para todos os índices em LMW
        flags = ['C'] * len(LMW)

        # Verificar onde LMW não é NaN e substituir os valores correspondentes em flags por 'C'
        nan_indices = np.where(np.isnan(LMW))[0]
        flags = list(flags)  # Converter flags para uma lista
        for idx in nan_indices:
            flags[idx] = 'S'

        # Usar idx_total como índices para transformar os valores de flags em 'S'
        for idx in idx_total:
            if idx < len(flags):
                flags[idx] = 'S'

        for idx in idx_total:
            print("L1-L2 (+)", idx, timep.iloc[idx])


        # ----------------- [L1 - L5]
        idx_total15 = []

        for arc in arcs15:
            arc_data = df.iloc[arc]
            time = df.index[arc]

            if len(arc_data) < ARCL:  # Verifica se há pontos suficientes para o ajuste
                continue

            x = arc_data.index.astype(np.int64) // 10**9  # Convertendo para segundos
            xx = arc_data['time']
            y = arc_data['LMW3'].values

            y_rescaled = rescale_data(y)
            delta_y = np.diff(y_rescaled, prepend=np.nan)

            # # Ajustar um polinômio apenas nos valores válidos (excluindo np.nan)
            p = Polynomial.fit(x[1:], delta_y[1:], 3)

            # print('AQUI 2:', len(delta_y), len(p), len(y))

            delta_y_fit = p(x)  # Valores ajustados pelo polinômio
            residuals = abs(delta_y - delta_y_fit)  # Calcular resíduos

            # print('AQUI 2:', len(delta_y), len(p), len(y), len(delta_y_fit), len(residuals))

            # Cálculo dos quartis e IQR para identificação de outliers
            Q1 = np.nanpercentile(residuals, 15)
            Q3 = np.nanpercentile(residuals, 85)
            IQR = Q3 - Q1
            outlier_mask = (residuals < Q1 - 4 * IQR) | (residuals > Q3 + 4 * IQR)
            high_residuals_mask = residuals > 1  # Máscara para resíduos altos
            other_residuals_mask = ~(outlier_mask | high_residuals_mask)  # Máscara para os demais resíduos

            # Configurar subplots
            fig, axs = plt.subplots(3, 1, figsize=(10, 15), sharex=True)

            # Primeiro subplot: dados originais reescalados
            axs[0].scatter(time, y_rescaled, label='Dados Reescalados')
            axs[0].set_title(f'Análise do Arco {arcs15.index(arc) + 1} - Satélite: {sat} - L1-L5' )
            axs[0].set_ylabel('LMW 1-5 (Reescalado)')
            axs[0].legend()

            # Segundo subplot: diferenças ponto a ponto e polinômio ajustado
            axs[1].scatter(time, delta_y, color='green', label='Diferença Ponto a Ponto', marker='o')
            axs[1].plot(time, delta_y_fit, color='red', label='Polinômio de 3º Grau Ajustado')
            axs[1].axhline(0, color='gray', linestyle='--')
            axs[1].set_ylabel('Delta Y (Reescalado)')
            axs[1].legend()
            # plt.grid(axis='both', linestyle='--', color='black', linewidth=0.5)

            # Terceiro subplot: resíduos com outliers e resíduos altos
            axs[2].scatter(time[other_residuals_mask], residuals[other_residuals_mask], color='purple', label='Resíduos Normais', marker='o')
            axs[2].scatter(time[outlier_mask], residuals[outlier_mask], color='green', label='Outliers', marker='o')
            axs[2].scatter(time[high_residuals_mask], residuals[high_residuals_mask], color='red', label='Resíduos Altos (> 1)', marker='o')


            axs[0].scatter(time[outlier_mask], y_rescaled[outlier_mask], color='green', label='Outliers', marker='o')
            axs[0].scatter(time[high_residuals_mask], y_rescaled[high_residuals_mask], color='red', label='Resíduos Altos (> 1)', marker='o')
            # plt.grid(axis='both', linestyle='--', color='black', linewidth=0.5)

            axs[2].axhline(0, color='gray', linestyle='--')
            # axs[2].set_ylim(0, 10)  # Definir os limites do eixo y para os resíduos
            axs[2].set_xlabel('Tempo (s)')
            axs[2].set_ylabel('Resíduos')
            axs[2].legend()

            plt.grid(axis='both', linestyle='--', color='black', linewidth=0.5)
            hours_fmt = mdates.DateFormatter('%H:%M')
            minute_locator = mdates.MinuteLocator(interval=int1)
            plt.gca().xaxis.set_major_formatter(hours_fmt)
            plt.gca().xaxis.set_major_locator(minute_locator)
            hora_inicio = h1 * 240
            start_time = np.datetime64(obs_data.time[hora_inicio].to_numpy())
            # # plt.xlim([start_time, start_time + np.timedelta64(n_horas, 'h')])
            plt.tick_params(axis='both', which='major', labelsize=12)
            # plt.show()

            # Suponha que 'residuos' seja a variável que contém os resíduos calculados
            limiar_outlier = 4  # Limiar para identificar outliers (por exemplo, 4 vezes o IQR)
            limiar_residuo_alto = 1  # Limiar para identificar resíduos altos

            # Identificar índices de outliers e resíduos altos
            indices_outliers15 =  arc[0] + np.where(np.abs(residuals) > limiar_outlier * IQR)[0]
            indices_residuos_altos15 = arc[0] + np.where(np.abs(residuals) > limiar_residuo_alto)[0]

            # Juntar os índices sem repeti-los
            indices_combinados15 = np.union1d(indices_outliers15, indices_residuos_altos15)

            # print("Índices combinados:", indices_combinados)

            # Adicionar os valores de indices_combinados à lista
            idx_total15.append(indices_combinados15)

            print("Índices combinados (L1 - L5):", indices_combinados15)#, timep.iloc[indices_combinados])

        # Concatenar todos os valores da lista em uma única array numpy
        if idx_total15:
            idx_total15 = np.concatenate(idx_total15)

        # Verificar se LMW15 não é completamente formada por NaN
        if not np.all(np.isnan(LMW15)):
            # Verificar onde LMW não é NaN e substituir os valores correspondentes em flags por 'C'
            nan_indices15 = np.where(np.isnan(LMW15))[0]

            for idx in nan_indices15:
                flags[idx] = 'S'


        # Usar idx_total como índices para transformar os valores de flags em 'S'
        for idx in idx_total15:
            if idx < len(flags):
                flags[idx] = 'S'

        for idx in idx_total15:
            print("L1-L5 (+)", idx, timep.iloc[idx])

        satellite_values = [sat] * len(L1)
        station = [station_name.upper()] * len(L1)
        position = [obs_data.position] * len(L1)

        L1 = [value if pd.notna(value) else -999999.999 for value in L1]
        L2 = [value if pd.notna(value) else -999999.999 for value in L2]
        L5 = [value if pd.notna(value) else -999999.999 for value in L5]

        P1 = [value if pd.notna(value) else -999999.999 for value in P1]
        P2 = [value if pd.notna(value) else -999999.999 for value in P2]
        P5 = [value if pd.notna(value) else -999999.999 for value in P5]

        L1 = ["{:.3f}".format(valor) for valor in L1]
        L2 = ["{:.3f}".format(valor) for valor in L2]
        L5 = ["{:.3f}".format(valor) for valor in L5]

        P1 = ["{:.3f}".format(valor) for valor in P1]
        P2 = ["{:.3f}".format(valor) for valor in P2]
        P5 = ["{:.3f}".format(valor) for valor in P5]

        print(sat)

        df.reset_index(drop=True, inplace=True)
        combined_df.reset_index(drop=True, inplace=True)

        export_df = pd.DataFrame({
            'date': df['date'],
            'time': df['time'],
            'mjd': df['mjd'],
            'pos_x': df['pos_x'],
            'pos_y': df['pos_y'],
            'pos_z': df['pos_z'],
            'L1': df['L1'],
            'L2': df['L2'],
            'L5': df['L5'],
            'P1': df['P1'],
            'P2': df['P2'],
            'P5': df['P5'],
            'cs_flags': flags,
            'satellite': satellite_values,
            'sta': station,
            'hght': df['height'],
            'El': abs(combined_df['Elevation'].round(2)),
            'Lon': combined_df['Longitude'],
            'Lat': combined_df['Latitude'],
        })

        export_df.rename(columns={'P1': code_obs1}, inplace=True)
        export_df.rename(columns={'P2': code_obs2}, inplace=True)
        export_df.rename(columns={'P5': code_obs5}, inplace=True)

        file_name = f"{station_name.upper()}_{sat}_{day_of_year}_{year}.RNX1"

        output_directory = os.path.join(output_folder, station_name.upper())
        full_path = output_directory


        os.makedirs(full_path, exist_ok=True)
        output_file_path = os.path.join(full_path, file_name)
        export_df.to_csv(output_file_path, sep='\t', index=False, na_rep='-999999.999')
        print(f"Dados exportados para {output_file_path}.")
        #plt.show()


